# -*- coding: utf-8 -*-
"""MyNEOAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cuzLJkzLXuLhHeryc6XppnQwRM-QLRE1

May 25th
CHANGES ADAMW and MIXED PRECISION

May 28th
CHANGES LEARNING RATE SCHEDULER

July 19th
ADDED RANDOMDATA1 - trying to fix it

Oct 22nd
TRYING TO FIX RANDOMDATA1 AND INTRODUCE CONVERSATIONAL DATA

Nov 1st -- INTRODUCING NEW MODELS
"""

import os
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

import torch
from torch.utils.data import DataLoader, Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW, get_linear_schedule_with_warmup
from google.colab import drive
import json
from torch.cuda.amp import GradScaler, autocast
import copy

# Mount Google Drive
drive.mount('/drive')

# File paths
file_path1 = '/drive/MyDrive/HenriAI/data.json'
file_path2 = '/drive/MyDrive/HenriAI/commonsense_data.json'
file_path3 = '/drive/MyDrive/HenriAI/general_chatbot_knowledge.json'
file_path4 = '/drive/MyDrive/HenriAI/randomData1.json'

# Load data function
def load_json(file_path):
    try:
        with open(file_path, 'r') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"Error loading {file_path}: {e}")
        return []

data = load_json(file_path1)
commonsense_data = load_json(file_path2)
general_chatbot_knowledge = load_json(file_path3)
random_data = load_json(file_path4)

# Copy the desired data into another set for fine-tuning
target_data = copy.deepcopy(data)

# Data processing function with enhanced error handling
def process_data_entry(entry):
    if 'question' in entry and 'answer' in entry:
        question = entry['question']
        answer = entry['answer']

        # Ensure 'answer' is a string
        if isinstance(answer, list):
            answer = " ".join(answer)  # Join list elements if needed

        return [{"input": question, "output": answer}]

    elif 'context' in entry and 'response' in entry:
        context_str = '\n'.join([f"{msg['role']}: {msg['text']}" for msg in entry['context']])
        response = entry['response']

        # Ensure 'response' is a string
        if isinstance(response, list):
            response = " ".join(response)

        return [{"input": context_str.strip(), "output": response}]

    return []

# Augment dataset
augmented_data = []
all_datasets = [data, commonsense_data, general_chatbot_knowledge, random_data]
for dataset in all_datasets:
    for entry in dataset:
        augmented_data.extend(process_data_entry(entry))

# Device setup
device = "cuda" if torch.cuda.is_available() else "cpu"

# Initialize the tokenizer and model for GPT-J 6B
tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-j-6B')
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-j-6B')
model.gradient_checkpointing_enable()  # Enable gradient checkpointing
model.to(device)
model.train()

# Dataset class
class QADataset(Dataset):
    def __init__(self, data, tokenizer, max_length=512):
        self.tokenizer = tokenizer
        self.inputs = []
        self.attn_masks = []
        for item in data:
            combined_text = item['input'] + "\n" + item['output'] + tokenizer.eos_token
            encodings_dict = self.tokenizer(
                combined_text,
                truncation=True,
                max_length=max_length,
                padding="max_length"
            )
            self.inputs.append(torch.tensor(encodings_dict['input_ids']))
            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))
    def __len__(self):
        return len(self.inputs)
    def __getitem__(self, idx):
        return self.inputs[idx], self.attn_masks[idx]

# Processed target_data for fine-tuning
processed_target_data = [process_data_entry(entry) for entry in target_data]
processed_target_data = [item for sublist in processed_target_data for item in sublist]

# Initialize DataLoaders
dataset = QADataset(augmented_data, tokenizer)
target_dataset = QADataset(processed_target_data, tokenizer)
loader = DataLoader(dataset, batch_size=1, shuffle=True)
target_loader = DataLoader(target_dataset, batch_size=1, shuffle=True)

# Optimizer and Scheduler
optimizer = AdamW(model.parameters(), lr=5e-5)
total_steps = len(loader) * 10 + len(target_loader) * 5
scheduler = get_linear_schedule_with_warmup(
    optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps
)

# Mixed Precision Training with GradScaler
scaler = GradScaler()

# Training Loop
epochs = 10
for epoch in range(epochs):
    total_loss = 0
    for inputs, masks in loader:
        inputs, masks = inputs.to(device), masks.to(device)
        optimizer.zero_grad()
        with autocast():
            outputs = model(inputs, attention_mask=masks, labels=inputs)
            loss = outputs.loss
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        scheduler.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(dataset)
    print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

# Fine-Tuning Loop
fine_tune_epochs = 5
for epoch in range(fine_tune_epochs):
    total_loss = 0
    for inputs, masks in target_loader:
        inputs, masks = inputs.to(device), masks.to(device)
        optimizer.zero_grad()
        with autocast():
            outputs = model(inputs, attention_mask=masks, labels=inputs)
            loss = outputs.loss
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        scheduler.step()
        total_loss += loss.item()
    avg_loss = total_loss / len(target_dataset)
    print(f"Fine-Tune Epoch {epoch+1}/{fine_tune_epochs}, Loss: {avg_loss:.4f}")

# Save Model and Tokenizer
save_directory = '/drive/MyDrive/HenriAI/gptJ6B'
os.makedirs(save_directory, exist_ok=True)

model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)